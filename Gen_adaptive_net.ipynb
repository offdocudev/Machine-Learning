{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gen adaptive net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/offdocudev/Machine-Learning/blob/master/Gen_adaptive_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QojeZPwx2IJ2",
        "colab_type": "code",
        "outputId": "2f556e24-c71c-4c6f-bfdd-63e51bd0217c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import math\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# DATA - https://www.kaggle.com/c/digit-recognizer/data\n",
        "# Kaggle \n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "#! cp kaggle.json ~/.kaggle/\n",
        "# Or manually copy credentials in kaggle.json file \n",
        "\n",
        "!  echo '{\"username\":\"offdocudev\",\"key\":\"a6893bc3aa38aec8a688959083e82644\"}' > /root/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "! cat /root/.kaggle/kaggle.json\n",
        "! chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "#################### DOWNLOAD AND UNZIP FILE SAVED IN DRIVE ####################\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "# HERE YOUR FILE ID ( GET IT WITH THE SHARING URL: https://drive.google.com/open?id=1Soh3zXLXt2lT7b_3FcWWyeOCC7SnOxK0 )\n",
        "zip_id = '1Soh3zXLXt2lT7b_3FcWWyeOCC7SnOxK0'\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import zipfile, os\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n",
            "{\"username\":\"offdocudev\",\"key\":\"a6893bc3aa38aec8a688959083e82644\"}\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 20.1MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NncAR7Tl2JYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input,Dense,Flatten,Dropout,Reshape,Concatenate,BatchNormalization,Conv2D,Conv2DTranspose,Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vup3_843nhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5cOLxjP3pVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_generator(input_layer,target_layer):\n",
        "  merged_layer = Concatenate()([input_layer,target_layer])\n",
        "  hid = Dense(128 * 8 * 8,activation='relu')(merged_layer)\n",
        "  hid = BatchNormalization(momentum = 0.9)(hid)\n",
        "  hid = LeakyReLU(alpha = 0.1)(hid)\n",
        "  hid = Reshape((8,8,128))(hid)\n",
        "  \n",
        "  \n",
        "  hid = Conv2D(128,kernel_size = 5,strides = 1,padding = 'same' )(hid)\n",
        "  hid = BatchNormalization(momentum = 0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "  \n",
        "  hid = Conv2DTranspose(128,kernel_size = 4,strides =2,padding= 'same')(hid)\n",
        "  hid = BatchNormalization(momentum = 0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "  \n",
        "  hid = Conv2D(128,kernel_size=5,strides = 1,padding ='same')(hid)\n",
        "  hid = BatchNormalization(momentum = 0.9)(hid)\n",
        "  hid = LeakyReLU(alpha =0.1)(hid)\n",
        "  \n",
        "  hid = Conv2DTranspose(128,kernel_size=4,strides=2,padding = 'same')(hid)\n",
        "  hid = BatchNormalization(momentum =0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "  \n",
        "  hid = Conv2D(128, kernel_size=5, strides=1, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Conv2D(128, kernel_size=5, strides=1, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "                      \n",
        "  hid = Conv2D(3, kernel_size=5, strides=1, padding=\"same\")(hid)\n",
        "  out = Activation(\"tanh\")(hid)\n",
        "\n",
        "  model = Model(inputs=[input_layer, target_layer], outputs=out)\n",
        "  model.summary()\n",
        "  \n",
        "  return model, out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TODiE9mE5Nn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_discriminator(input_layer,target_layer):\n",
        "  hid = Conv2D(128,kernel_size= 3,strides =1,padding = 'same')(input_layer)\n",
        "  hid = BatchNormalization(momentum = 0.9)(hid)\n",
        "  hid = LeakyReLU(alpha =0.1)(hid)\n",
        "  \n",
        "  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Flatten()(hid)\n",
        "  \n",
        "  merged_layer = Concatenate()([hid,target_layer])\n",
        "  hid = Dense(512,activation='relu')(merged_layer)\n",
        "  \n",
        "  out = Dense(1,activation='sigmoid')(hid)\n",
        "  model = Model(inputs = [input_layer,target_layer],outputs = out)\n",
        "  model.summary()\n",
        "  \n",
        "  return model,out\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6Q4RtF76Do7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "def one_Hot(y):\n",
        "  z = np.zeros((len(y),10))\n",
        "  idx = np.arange(len(y))\n",
        "  z[idx,y] = 1\n",
        "  return z\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R3NN7Sc6XqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_noise(n_samples,noise_dim):\n",
        "  X = np.random.normal(0,1,size = (n_samples,noise_dim))\n",
        "  return X\n",
        "\n",
        "def generate_random_labels(n):\n",
        "  y = np.random.choice(10,n)\n",
        "  y = one_Hot(y)\n",
        "  return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbjGZNB86sma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkQLe2H7KyBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "def one_hot_encode(y):\n",
        "  z = np.zeros((len(y), 10))\n",
        "  idx = np.arange(len(y))\n",
        "  z[idx, y] = 1\n",
        "  return z\n",
        "\n",
        "def generate_noise(n_samples, noise_dim):\n",
        "  X = np.random.normal(0, 1, size=(n_samples, noise_dim))\n",
        "  return X\n",
        "\n",
        "def generate_random_labels(n):\n",
        "  y = np.random.choice(10, n)\n",
        "  y = one_hot_encode(y)\n",
        "  return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNUpp7cQ6vz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_samples(batchidx):\n",
        "  fig,axs = plt.subplots(5,6,figsize = (10,10))\n",
        "  plt.subplots_adjust(hspace=0.3,wspace=0.1)\n",
        "  \n",
        "  for classlabel in range(10):\n",
        "    row = int(classlabel /2)\n",
        "    col = (classlabel % 2 ) *3\n",
        "    lbls = one_hot_encode([classlabel ] * 3)\n",
        "    noise = generate_noise(3,100)\n",
        "    gen_imgs = generator.predict([noise,lbls])\n",
        "    \n",
        "    for i in range(3):\n",
        "      img = image.array_to_img(gen_imgs[i],scale = True)\n",
        "      axs[row,col+i].imshow(img)\n",
        "      axs[row,i+col].axis('off')\n",
        "      \n",
        "      if i==1:\n",
        "        axs[row,i+col].set_title(tags[classlabel])\n",
        "        \n",
        "plt.show()\n",
        "plt.close()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z40YVlqM70VL",
        "colab_type": "code",
        "outputId": "abb99c8a-50e3-4fe2-dcdd-0bf51de84405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2247
        }
      },
      "source": [
        "img_input = Input(shape=(32,32,3))\n",
        "disc_condition_input = Input(shape=(10,))\n",
        "\n",
        "discriminator, disc_out = get_discriminator(img_input, disc_condition_input)\n",
        "discriminator.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "noise_input = Input(shape=(100,))\n",
        "gen_condition_input = Input(shape=(10,))\n",
        "generator, gen_out = get_generator(noise_input, gen_condition_input)\n",
        "\n",
        "gan_input = Input(shape=(100,))\n",
        "x = generator([gan_input, gen_condition_input])\n",
        "gan_out = discriminator([x, disc_condition_input])\n",
        "gan = Model(inputs=[gan_input, gen_condition_input, disc_condition_input], output=gan_out)\n",
        "gan.summary()\n",
        "\n",
        "gan.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 128)  3584        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 128)  262272      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 8, 8, 128)    262272      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 8, 8, 128)    512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 4, 4, 128)    262272      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 4, 4, 128)    512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 128)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 2048)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2058)         0           flatten_1[0][0]                  \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          1054208     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            513         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,847,169\n",
            "Trainable params: 1,846,145\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 110)          0           input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 8192)         909312      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8192)         32768       dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 8192)         0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 8, 8, 128)    0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 8, 8, 128)    409728      reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 8, 8, 128)    512         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 8, 8, 128)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 16, 16, 128)  262272      leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 128)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 128)  409728      leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 128)  512         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 128)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 32, 32, 128)  262272      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 128)  409728      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 128)  512         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 32, 32, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 128)  409728      leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 128)  512         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 32, 32, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 3)    9603        leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 3)    0           conv2d_9[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 3,118,211\n",
            "Trainable params: 3,100,291\n",
            "Non-trainable params: 17,920\n",
            "__________________________________________________________________________________________________\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_2 (Model)                 (None, 32, 32, 3)    3118211     input_5[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Model)                 (None, 1)            1847169     model_2[1][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 4,965,380\n",
            "Trainable params: 3,100,291\n",
            "Non-trainable params: 1,865,089\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"mo...)`\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt8AG9yl9WEv",
        "colab_type": "code",
        "outputId": "afaf21fa-9fd6-4406-d768-22a6ea7d0d23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "\n",
        "# # Get training images\n",
        "(X_train, y_train), (X_test, _) = cifar10.load_data()\n",
        "\n",
        "# Normalize data\n",
        "X_train = (X_train - 127.5) / 127.5\n",
        "\n",
        "# 1hot encode labels\n",
        "y_train = one_hot_encode(y_train[:,0])\n",
        "\n",
        "print (\"Training shape: {}\".format(X_train.shape))\n",
        " \n",
        "num_batches = int(X_train.shape[0]/BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 44s 0us/step\n",
            "Training shape: (50000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqm-kNDpKR5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exp_replay = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxFRxA0V_uWw",
        "colab_type": "code",
        "outputId": "bd34ccf5-84a2-4b82-ac7d-a83d24bb5d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "N_EPOCHS = 10\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  cum_d_loss = 0.\n",
        "  cum_g_loss = 0.\n",
        "  \n",
        "  for batch_idx in range(num_batches):\n",
        "    # Get the next set of real images to be used in this iteration\n",
        "    images = X_train[batch_idx*BATCH_SIZE : (batch_idx+1)*BATCH_SIZE]\n",
        "    labels = y_train[batch_idx*BATCH_SIZE : (batch_idx+1)*BATCH_SIZE]\n",
        "\n",
        "    noise_data = generate_noise(BATCH_SIZE, 100)\n",
        "    random_labels = generate_random_labels(BATCH_SIZE)\n",
        "    # We use same labels for generated images as in the real training batch\n",
        "    generated_images = generator.predict([noise_data, labels])\n",
        "\n",
        "    # Train on soft targets (add noise to targets as well)\n",
        "    noise_prop = 0.05 # Randomly flip 5% of targets\n",
        "    \n",
        "    # Prepare labels for real data\n",
        "    true_labels = np.zeros((BATCH_SIZE, 1)) + np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n",
        "    flipped_idx = np.random.choice(np.arange(len(true_labels)), size=int(noise_prop*len(true_labels)))\n",
        "    true_labels[flipped_idx] = 1 - true_labels[flipped_idx]\n",
        "    \n",
        "    # Train discriminator on real data\n",
        "    d_loss_true = discriminator.train_on_batch([images, labels], true_labels)\n",
        "\n",
        "    # Prepare labels for generated data\n",
        "    gene_labels = np.ones((BATCH_SIZE, 1)) - np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n",
        "    flipped_idx = np.random.choice(np.arange(len(gene_labels)), size=int(noise_prop*len(gene_labels)))\n",
        "    gene_labels[flipped_idx] = 1 - gene_labels[flipped_idx]\n",
        "    \n",
        "    # Train discriminator on generated data\n",
        "    d_loss_gene = discriminator.train_on_batch([generated_images, labels], gene_labels)\n",
        "    \n",
        "    # Store a random point for experience replay\n",
        "    r_idx = np.random.randint(BATCH_SIZE)\n",
        "    exp_replay.append([generated_images[r_idx], labels[r_idx], gene_labels[r_idx]])\n",
        "    \n",
        "    #If we have enough points, do experience replay\n",
        "    if len(exp_replay) == BATCH_SIZE:\n",
        "      generated_images = np.array([p[0] for p in exp_replay])\n",
        "      labels = np.array([p[1] for p in exp_replay])\n",
        "      gene_labels = np.array([p[2] for p in exp_replay])\n",
        "      expprep_loss_gene = discriminator.train_on_batch([generated_images, labels], gene_labels)\n",
        "      exp_replay = []\n",
        "      break\n",
        "    \n",
        "    d_loss = 0.5 * np.add(d_loss_true, d_loss_gene)\n",
        "    cum_d_loss += d_loss\n",
        "\n",
        "    # Train generator\n",
        "    noise_data = generate_noise(BATCH_SIZE, 100)\n",
        "    random_labels = generate_random_labels(BATCH_SIZE)\n",
        "    g_loss = gan.train_on_batch([noise_data, random_labels, random_labels], np.zeros((BATCH_SIZE, 1)))\n",
        "    cum_g_loss +=  g_loss\n",
        "\n",
        "  print('\\tEpoch: {}, Generator Loss: {}, Discriminator Loss: {}'.format(epoch+1, cum_g_loss/num_batches, cum_d_loss/num_batches))\n",
        "  show_samples(\"epoch\" + str(epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEpoch: 1, Generator Loss: 2.150850628512268, Discriminator Loss: [0.51420313 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY_vNKBADS3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}