{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fr-eng.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/offdocudev/Machine-Learning/blob/master/fr_eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3waIgc_BOLn",
        "colab_type": "code",
        "outputId": "a8f690d4-e78f-41d5-bf01-e6bc11b1a5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import math\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# DATA - https://www.kaggle.com/c/digit-recognizer/data\n",
        "# Kaggle \n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "#! cp kaggle.json ~/.kaggle/\n",
        "# Or manually copy credentials in kaggle.json file \n",
        "\n",
        "!  echo '{\"username\":\"offdocudev\",\"key\":\"a6893bc3aa38aec8a688959083e82644\"}' > /root/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "! cat /root/.kaggle/kaggle.json\n",
        "! chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "#################### DOWNLOAD AND UNZIP FILE SAVED IN DRIVE ####################\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "# HERE YOUR FILE ID ( GET IT WITH THE SHARING URL: https://drive.google.com/open?id=1Soh3zXLXt2lT7b_3FcWWyeOCC7SnOxK0 )\n",
        "zip_id = '1Soh3zXLXt2lT7b_3FcWWyeOCC7SnOxK0'\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import zipfile, os\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.0)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "{\"username\":\"offdocudev\",\"key\":\"a6893bc3aa38aec8a688959083e82644\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT4OUW8UAWeu",
        "colab_type": "code",
        "outputId": "8f9db7c6-39cc-447c-b6d3-33d144580ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!kaggle datasets download -d jannesklaas/frenchenglish-bilingual-pairs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frenchenglish-bilingual-pairs.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zClyTCjxBQzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_ref = zipfile.ZipFile('frenchenglish-bilingual-pairs.zip','r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpY9R-vEDUQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_ref = zipfile.ZipFile('fra-eng.zip','r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VILNixUOBYfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm_pS1hqBn69",
        "colab_type": "code",
        "outputId": "4a7738df-b3b3-4fc9-b1af-cf672bdfcc85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(os.listdir('/content'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', '_about.txt', 'fra.txt', 'fra-eng.zip', 'frenchenglish-bilingual-pairs.zip', 'adc.json', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drBdkKX3Bpqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.layers import CuDNNLSTM,Dense,Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2K3OrPeCZOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = 'fra.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eJfyxJWCgJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(data_path,delimiter= '\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yuq5MtaOR-Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = open(data_path).read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b0mS9DgR4Xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_samples = 10000\n",
        "input_texts =[]\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7R8Hg8WSVVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in lines[:min(num_samples,len(lines) -1)]:\n",
        "  \n",
        "  input_text,target_text = line.split('\\t')\n",
        "  input_texts.append(input_text)\n",
        "  target_text = '\\t' + target_text + '\\n'\n",
        "  target_texts.append(target_text)\n",
        "  \n",
        "  for char in input_text:\n",
        "    if char not in input_characters:\n",
        "      input_characters.add(char)\n",
        "      \n",
        "  for char in target_text:\n",
        "    if char not in target_characters:\n",
        "      target_characters.add(char)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1eo6a18TJDy",
        "colab_type": "code",
        "outputId": "6761b3d4-2c57-46b6-869c-f22f06eab656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1241
        }
      },
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "num_encoder = len(input_characters)\n",
        "num_decoder = len(target_characters)\n",
        "\n",
        "print(num_encoder,num_decoder)\n",
        "\n",
        "input_characters\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71 93\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '9',\n",
              " ':',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '’']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4YrIlK9Tg5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_index = {char : i for i,char in enumerate(input_characters)}\n",
        "target_index = {char : i for i,char in enumerate(target_characters)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53igUa4BTzql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_encoder = max([len(txt)  for txt in input_texts])\n",
        "max_decoder = max([len(txt) for txt in target_texts])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr4h8Ro5UFuq",
        "colab_type": "code",
        "outputId": "ba67cb83-8642-4210-9fcc-9d30bdf38419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(max_encoder,max_decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq8Ofhj3UH1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_data = np.zeros((len(input_texts),max_encoder,num_encoder),dtype='float32')\n",
        "decoder_input_data = np.zeros((len(input_texts),max_decoder,num_decoder),dtype='float32')\n",
        "decoder_target_data = np.zeros((len(target_texts),max_decoder,num_decoder),dtype='float32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJBoCCyDUn_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
        "  for t,char in enumerate(input_text):\n",
        "    encoder_input_data[i,t,input_index[char]] = 1\n",
        "    \n",
        "  for t,char in enumerate(target_text):\n",
        "    \n",
        "    decoder_input_data[i,t,target_index[char]] = 1\n",
        "    \n",
        "    if t>0:\n",
        "      decoder_target_data[i,t-1,target_index[char]] =1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3EYzgEqVdng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense,LSTM,CuDNNLSTM,Input\n",
        "latent_dim =512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXVp3BYHWB-T",
        "colab_type": "code",
        "outputId": "4574c79b-0b6c-4cb7-b7c3-f7e2ed78dc91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "encoder_input = Input(shape = (None,num_encoder))\n",
        "\n",
        "encoder = CuDNNLSTM(latent_dim,return_state=True)\n",
        "encoder_output,state_h,state_c = encoder(encoder_input)\n",
        "encoder_states = [state_h,state_c]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbRy7mBfZYty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFF0DSHDW5Pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_input = Input(shape = (None,num_decoder))\n",
        "decoder = CuDNNLSTM(latent_dim,return_state=True,return_sequences=True)\n",
        "decoder_output,_,_ = decoder(decoder_input,initial_state = encoder_states)\n",
        "\n",
        "decoder_dense = Dense(num_decoder,activation='softmax',)\n",
        "decoder_output = decoder_dense(decoder_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENGjAiZUXkny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model([encoder_input,decoder_input],decoder_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zTeINZBZZ_4",
        "colab_type": "code",
        "outputId": "73792a3a-e709-475c-8556-dbc90912f1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3556
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics =['acc'])\n",
        "history = model.fit([encoder_input_data, decoder_input_data], \n",
        "                    decoder_target_data,\n",
        "                    batch_size=1024,\n",
        "                    epochs=100,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 1.2267 - acc: 0.0328 - val_loss: 1.1875 - val_acc: 0.0567\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 4s 447us/step - loss: 0.9843 - acc: 0.0498 - val_loss: 1.1106 - val_acc: 0.0826\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 4s 438us/step - loss: 0.9517 - acc: 0.0565 - val_loss: 1.0954 - val_acc: 0.0711\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 4s 472us/step - loss: 0.9385 - acc: 0.0566 - val_loss: 1.1013 - val_acc: 0.0700\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 4s 463us/step - loss: 0.9271 - acc: 0.0606 - val_loss: 1.1008 - val_acc: 0.0572\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 4s 484us/step - loss: 0.9159 - acc: 0.0631 - val_loss: 1.0797 - val_acc: 0.0759\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 4s 479us/step - loss: 0.9064 - acc: 0.0647 - val_loss: 1.0712 - val_acc: 0.0654\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 3s 432us/step - loss: 0.9007 - acc: 0.0698 - val_loss: 1.0505 - val_acc: 0.0799\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 4s 460us/step - loss: 0.8816 - acc: 0.0731 - val_loss: 1.0216 - val_acc: 0.0947\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 4s 499us/step - loss: 0.8681 - acc: 0.0826 - val_loss: 1.0003 - val_acc: 0.1023\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 429us/step - loss: 0.8423 - acc: 0.0916 - val_loss: 0.9820 - val_acc: 0.1129\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 4s 481us/step - loss: 0.8241 - acc: 0.0953 - val_loss: 0.9405 - val_acc: 0.1220\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 4s 456us/step - loss: 0.7988 - acc: 0.1014 - val_loss: 0.9254 - val_acc: 0.1124\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 4s 459us/step - loss: 0.7837 - acc: 0.1039 - val_loss: 0.9138 - val_acc: 0.1128\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 4s 445us/step - loss: 0.7662 - acc: 0.1099 - val_loss: 0.8920 - val_acc: 0.1322\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 4s 485us/step - loss: 0.7443 - acc: 0.1153 - val_loss: 0.8620 - val_acc: 0.1376\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 4s 471us/step - loss: 0.7258 - acc: 0.1194 - val_loss: 0.8490 - val_acc: 0.1353\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 4s 485us/step - loss: 0.7152 - acc: 0.1205 - val_loss: 0.8318 - val_acc: 0.1413\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 4s 470us/step - loss: 0.6967 - acc: 0.1258 - val_loss: 0.8273 - val_acc: 0.1453\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 4s 468us/step - loss: 0.6876 - acc: 0.1266 - val_loss: 0.7957 - val_acc: 0.1498\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 4s 490us/step - loss: 0.6749 - acc: 0.1292 - val_loss: 0.7853 - val_acc: 0.1497\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 4s 496us/step - loss: 0.6613 - acc: 0.1318 - val_loss: 0.7718 - val_acc: 0.1524\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 4s 472us/step - loss: 0.6473 - acc: 0.1353 - val_loss: 0.7571 - val_acc: 0.1529\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 4s 446us/step - loss: 0.6394 - acc: 0.1362 - val_loss: 0.7712 - val_acc: 0.1557\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 4s 479us/step - loss: 0.6288 - acc: 0.1403 - val_loss: 0.7388 - val_acc: 0.1602\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 4s 497us/step - loss: 0.6194 - acc: 0.1416 - val_loss: 0.7513 - val_acc: 0.1569\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 4s 477us/step - loss: 0.6144 - acc: 0.1439 - val_loss: 0.7259 - val_acc: 0.1628\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 5s 596us/step - loss: 0.5975 - acc: 0.1474 - val_loss: 0.7244 - val_acc: 0.1629\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 7s 924us/step - loss: 0.5945 - acc: 0.1487 - val_loss: 0.7160 - val_acc: 0.1675\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 8s 958us/step - loss: 0.5918 - acc: 0.1493 - val_loss: 0.6979 - val_acc: 0.1714\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 7s 849us/step - loss: 0.5809 - acc: 0.1517 - val_loss: 0.7023 - val_acc: 0.1713\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 8s 946us/step - loss: 0.5715 - acc: 0.1544 - val_loss: 0.7062 - val_acc: 0.1659\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 7s 869us/step - loss: 0.5659 - acc: 0.1564 - val_loss: 0.6956 - val_acc: 0.1692\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 7s 930us/step - loss: 0.5603 - acc: 0.1580 - val_loss: 0.6798 - val_acc: 0.1767\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 7s 920us/step - loss: 0.5534 - acc: 0.1602 - val_loss: 0.6742 - val_acc: 0.1769\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 7s 872us/step - loss: 0.5465 - acc: 0.1613 - val_loss: 0.6618 - val_acc: 0.1806\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 7s 922us/step - loss: 0.5477 - acc: 0.1615 - val_loss: 0.6568 - val_acc: 0.1822\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 7s 858us/step - loss: 0.5304 - acc: 0.1666 - val_loss: 0.6467 - val_acc: 0.1855\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 7s 902us/step - loss: 0.5307 - acc: 0.1660 - val_loss: 0.6582 - val_acc: 0.1819\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 7s 928us/step - loss: 0.5227 - acc: 0.1692 - val_loss: 0.6652 - val_acc: 0.1789\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 7s 899us/step - loss: 0.5144 - acc: 0.1705 - val_loss: 0.6316 - val_acc: 0.1899\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 7s 933us/step - loss: 0.5119 - acc: 0.1715 - val_loss: 0.6415 - val_acc: 0.1856\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 5s 613us/step - loss: 0.5110 - acc: 0.1718 - val_loss: 0.6181 - val_acc: 0.1946\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 4s 443us/step - loss: 0.4947 - acc: 0.1763 - val_loss: 0.6156 - val_acc: 0.1948\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 4s 465us/step - loss: 0.4943 - acc: 0.1768 - val_loss: 0.6443 - val_acc: 0.1856\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 4s 499us/step - loss: 0.4966 - acc: 0.1763 - val_loss: 0.6158 - val_acc: 0.1939\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 4s 494us/step - loss: 0.4813 - acc: 0.1799 - val_loss: 0.6153 - val_acc: 0.1941\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 4s 481us/step - loss: 0.4798 - acc: 0.1803 - val_loss: 0.6062 - val_acc: 0.1983\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 4s 485us/step - loss: 0.4743 - acc: 0.1822 - val_loss: 0.5914 - val_acc: 0.1978\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 4s 469us/step - loss: 0.4691 - acc: 0.1834 - val_loss: 0.6035 - val_acc: 0.1972\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 4s 500us/step - loss: 0.4619 - acc: 0.1855 - val_loss: 0.6002 - val_acc: 0.1986\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 4s 491us/step - loss: 0.4638 - acc: 0.1856 - val_loss: 0.5838 - val_acc: 0.2037\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 4s 474us/step - loss: 0.4542 - acc: 0.1873 - val_loss: 0.5879 - val_acc: 0.2023\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 4s 476us/step - loss: 0.4482 - acc: 0.1894 - val_loss: 0.5955 - val_acc: 0.1992\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 4s 493us/step - loss: 0.4442 - acc: 0.1906 - val_loss: 0.5769 - val_acc: 0.2062\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 4s 477us/step - loss: 0.4419 - acc: 0.1907 - val_loss: 0.5733 - val_acc: 0.2048\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 4s 445us/step - loss: 0.4352 - acc: 0.1929 - val_loss: 0.5782 - val_acc: 0.2045\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 4s 471us/step - loss: 0.4279 - acc: 0.1951 - val_loss: 0.5667 - val_acc: 0.2057\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 4s 501us/step - loss: 0.4268 - acc: 0.1949 - val_loss: 0.5729 - val_acc: 0.2055\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 4s 483us/step - loss: 0.4230 - acc: 0.1962 - val_loss: 0.5659 - val_acc: 0.2085\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 4s 448us/step - loss: 0.4136 - acc: 0.1986 - val_loss: 0.5655 - val_acc: 0.2087\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 4s 446us/step - loss: 0.4154 - acc: 0.1984 - val_loss: 0.5602 - val_acc: 0.2099\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 4s 477us/step - loss: 0.4063 - acc: 0.2006 - val_loss: 0.5504 - val_acc: 0.2126\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 4s 483us/step - loss: 0.4033 - acc: 0.2013 - val_loss: 0.5564 - val_acc: 0.2110\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 4s 454us/step - loss: 0.3962 - acc: 0.2040 - val_loss: 0.5665 - val_acc: 0.2071\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 437us/step - loss: 0.3944 - acc: 0.2040 - val_loss: 0.5460 - val_acc: 0.2130\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 4s 517us/step - loss: 0.3915 - acc: 0.2052 - val_loss: 0.5496 - val_acc: 0.2126\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 4s 469us/step - loss: 0.3831 - acc: 0.2074 - val_loss: 0.5419 - val_acc: 0.2153\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 4s 446us/step - loss: 0.3798 - acc: 0.2084 - val_loss: 0.5430 - val_acc: 0.2157\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 4s 456us/step - loss: 0.3779 - acc: 0.2086 - val_loss: 0.5467 - val_acc: 0.2154\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 4s 492us/step - loss: 0.3721 - acc: 0.2109 - val_loss: 0.5264 - val_acc: 0.2201\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 4s 471us/step - loss: 0.3728 - acc: 0.2100 - val_loss: 0.5291 - val_acc: 0.2179\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 4s 446us/step - loss: 0.3598 - acc: 0.2142 - val_loss: 0.5254 - val_acc: 0.2208\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 4s 463us/step - loss: 0.3594 - acc: 0.2145 - val_loss: 0.5346 - val_acc: 0.2173\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 4s 507us/step - loss: 0.3560 - acc: 0.2155 - val_loss: 0.5253 - val_acc: 0.2221\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 4s 445us/step - loss: 0.3512 - acc: 0.2168 - val_loss: 0.5254 - val_acc: 0.2214\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 4s 482us/step - loss: 0.3472 - acc: 0.2178 - val_loss: 0.5270 - val_acc: 0.2214\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 4s 451us/step - loss: 0.3440 - acc: 0.2189 - val_loss: 0.5241 - val_acc: 0.2203\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 4s 480us/step - loss: 0.3388 - acc: 0.2204 - val_loss: 0.5352 - val_acc: 0.2183\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 427us/step - loss: 0.3341 - acc: 0.2221 - val_loss: 0.5271 - val_acc: 0.2214\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 395us/step - loss: 0.3329 - acc: 0.2220 - val_loss: 0.5217 - val_acc: 0.2225\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 395us/step - loss: 0.3273 - acc: 0.2240 - val_loss: 0.5125 - val_acc: 0.2259\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 393us/step - loss: 0.3235 - acc: 0.2247 - val_loss: 0.5284 - val_acc: 0.2207\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 399us/step - loss: 0.3193 - acc: 0.2264 - val_loss: 0.5147 - val_acc: 0.2251\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 396us/step - loss: 0.3154 - acc: 0.2274 - val_loss: 0.5124 - val_acc: 0.2252\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 4s 498us/step - loss: 0.3146 - acc: 0.2278 - val_loss: 0.5159 - val_acc: 0.2250\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 4s 491us/step - loss: 0.3047 - acc: 0.2308 - val_loss: 0.5181 - val_acc: 0.2242\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 435us/step - loss: 0.3030 - acc: 0.2316 - val_loss: 0.5271 - val_acc: 0.2218\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 4s 475us/step - loss: 0.3021 - acc: 0.2311 - val_loss: 0.5258 - val_acc: 0.2230\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 4s 478us/step - loss: 0.2978 - acc: 0.2326 - val_loss: 0.5047 - val_acc: 0.2294\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 4s 462us/step - loss: 0.2910 - acc: 0.2345 - val_loss: 0.5144 - val_acc: 0.2259\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 4s 486us/step - loss: 0.2912 - acc: 0.2344 - val_loss: 0.5172 - val_acc: 0.2271\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 4s 476us/step - loss: 0.2871 - acc: 0.2354 - val_loss: 0.5133 - val_acc: 0.2275\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 4s 453us/step - loss: 0.2815 - acc: 0.2374 - val_loss: 0.5122 - val_acc: 0.2286\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 4s 477us/step - loss: 0.2817 - acc: 0.2371 - val_loss: 0.5196 - val_acc: 0.2265\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 4s 494us/step - loss: 0.2743 - acc: 0.2398 - val_loss: 0.5090 - val_acc: 0.2290\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 4s 459us/step - loss: 0.2710 - acc: 0.2407 - val_loss: 0.5095 - val_acc: 0.2293\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 4s 448us/step - loss: 0.2692 - acc: 0.2413 - val_loss: 0.5067 - val_acc: 0.2310\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 4s 484us/step - loss: 0.2660 - acc: 0.2422 - val_loss: 0.5198 - val_acc: 0.2259\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 4s 491us/step - loss: 0.2607 - acc: 0.2439 - val_loss: 0.5124 - val_acc: 0.2300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5FM05aYYyJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model(encoder_input,encoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vvIiyjubKt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_input_h = Input(shape = (latent_dim,))\n",
        "decoder_input_c = Input(shape = (latent_dim,))\n",
        "\n",
        "decoder_s_inputs = [decoder_input_h,decoder_input_c]\n",
        "\n",
        "decoder_output,states_h,states_c = decoder(decoder_input ,initial_state=decoder_s_inputs)\n",
        "decoder_states = [states_h,states_c]\n",
        "\n",
        "decoder_output = decoder_dense(decoder_output)\n",
        "\n",
        "decoder_model = Model([decoder_input]+decoder_s_inputs,[decoder_output] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-BLdzOScHnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_input_index = {i:char for char,i in input_index.items() } \n",
        "reverse_target_index = {i:char for char,i in target_index.items() } "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLQm3jWOdCyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder_sequence(input_seq):\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "  \n",
        "  target_seq = np.zeros((1,1,num_decoder))\n",
        "  \n",
        "  target_seq[0,0,target_index['\\t']] = 1\n",
        "  \n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  \n",
        "  while not stio_condition :\n",
        "    output_tokens,h,c = decoder_model.predict([target_seq] +states_value)\n",
        "    \n",
        "    sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
        "    \n",
        "    sampled_char = reverse_target_index[sampled_token_index]\n",
        "    \n",
        "    decoded_sentence += sampled_char\n",
        "    \n",
        "    if(sampled_char == '\\n' or len(decoded_sentence) > max_decoder ):\n",
        "      stop_condition = True\n",
        "      \n",
        "    target_seq = np.zeros((1,1,num_decoder))\n",
        "    target_sq[0,0,sampled_token_index] = 1.\n",
        "    \n",
        "    states_value = [h,c]\n",
        "    \n",
        "    \n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxJR2Z98ev0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "    # Loop untill we recieve a stop sign\n",
        "    while not stop_condition:\n",
        "        # Get output and internal states of the decoder \n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Get the predicted token (the token with the highest score)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        # Get the character belonging to the token\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        # Append char to output\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hncN3kj2ecY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_text = 'Thanks'\n",
        "placeholder = np.zeros((1,len(my_text)+10,num_encoder_tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enbNh2THeedl",
        "colab_type": "code",
        "outputId": "1c562adc-8c06-486b-d7dc-a950d0542190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "for i, char in enumerate(my_text):\n",
        "    print(i,char, input_token_index[char])\n",
        "    placeholder[0,i,input_token_index[char]] = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 T 31\n",
            "1 h 42\n",
            "2 a 35\n",
            "3 n 48\n",
            "4 k 45\n",
            "5 s 53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IKpLcvGef-q",
        "colab_type": "code",
        "outputId": "94dd48e8-3d56-467b-cd21-6f2812fa4940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        }
      },
      "source": [
        "decode_sequence(placeholder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-213-a800ecb01217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-210-cc93a195c1a5>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Encode the input as state vectors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstates_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Generate empty target sequence of length 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_22 to have shape (None, 71) but got array with shape (16, 61)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbX2t6wIejoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}